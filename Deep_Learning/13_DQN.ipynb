{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYlJSTarDe4LUlhWwP23x7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2018007956/HYU/blob/main/DeepLearning/13_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DQN architecture: CNN + FC  \n",
        "Environment setting: Gym"
      ],
      "metadata": {
        "id": "Sh-JOSGBjLT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "코드 수정 필요!!"
      ],
      "metadata": {
        "id": "RpNXgfcaoAeg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSebMvMEi786"
      },
      "outputs": [],
      "source": [
        "class DON (nn. Module)\n",
        "def __init__(self, h. w, outputs) :\n",
        "super (DON, self) .__init__(\n",
        "self.conv1 = nn .Conv2d(3. 16, kernel _size=5, stride=2)\n",
        "self.bn1 = nn. BatchNorm2d(16)\n",
        "self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "self.bn2 = nn. BatchNorm2d(32)\n",
        "self.conv3 = nn. Conv2d(32, 32. kernel_size=5, stride=2)\n",
        "self.bn3 = nn. BatchNorm2d (32)\n",
        "# Number of Linear input connections depends on output of conv2d layers\n",
        "# and therefore the input image size, so compute it.\n",
        "def conv2d_size_out (size, kernel_size = 5, stride = 2) :\n",
        "return (size - (kernel_size - 1) - 1) // stride + 1\n",
        "convw = conv2d_size_out (conv2d_size_out (conv2d_size_out (w) ))\n",
        "convh = conv2d_size_out (conv2d_size_out (conv2d_size_out (h)))\n",
        "linear_ input_ size = convw * convh * 32\n",
        "self. head = nn.Linear (linear_input _size, outputs)\n",
        "# Called with either one element to determine next action, or a batch\n",
        "# during optimization. Returns tensor (I[leftOexp, rightOexp]..\n",
        ".]) .\n",
        "def forward(self, x) :\n",
        "× = x. to (device)\n",
        "× = F.relu(self.bn1 (self.conv1 (x)))\n",
        "× = F.relu(self.bn2(self .conv2(x)))\n",
        "× = F.relu(self.bn3(self .conv3 (x)))\n",
        "return self.head(x. view(x. size (0) .\n",
        "-1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resize = T.Compose ([T. ToP IL| mage ().\n",
        "T.Resize (40. interpolation=|mage CUBIC).\n",
        "T.ToTensor (1)\n",
        "def get_cart_location (screen _width) :\n",
        "world width = env.× threshold * 2\n",
        "scale = screen _width / world width\n",
        "return int (env.state[0] * scale + screen _width / 2.0) # MIDDLE OF CART\n",
        "def get_screen:\n",
        "# Returned screen requested by gym is 400×600×3, but is sometimes larger # such as 800×1200×3. Transpose it into torch order (CHW).\n",
        "screen = env. render (mode= 'rgb _array'). transpose ((2, 0, 1))\n",
        "# Cart is in the lower half, so strip off the top and bottom of the screen\n",
        "-. screen _height, screen_width = screen. shape\n",
        "screen = screen[:, int(screen_height*0.4) :int (screen_height * 0.8)]\n",
        "view_width = int (screen _width * 0.6)\n",
        "cart_location = get _cart_location (screen_width)\n",
        "if cart_ location < view width // 2:\n",
        "slice_range = slice(view width)\n",
        "elif cart_location > (screen _width - view_width / / 2) :\n",
        "slice_range = slice(-view_width, None)\n",
        "else:\n",
        "slice_range = slice(cart_location - view_width // 2.\n",
        "cart_location + view_width // 2)\n",
        "# Strip off the edges, so that we have a square image centered on a cart\n",
        "screen = screen [:.\n",
        ". slice_range]\n",
        "# Convert to float, rescale, convert to torch tensor\n",
        "# (this doesn't require a copy)\n",
        "screen = np. ascontiguousarray(screen, dtype=np. float32) / 255\n",
        "screen = torch. from_numpy (screen)\n",
        "# Resize, and add a batch dimension (BCHW)\n",
        "return resize(screen) .unsqueeze(0)"
      ],
      "metadata": {
        "id": "p7Bema2tkVAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "# Get screen size so that we can initialize layers correctly based on shape\n",
        "# returned from Al gym. Typical dimensions at this point are close to 3x40×90\n",
        "# which is the result of a clamped and down-scaled render buffer in get_screen ()\n",
        "init_screen = get_screen ()\n",
        "screen_height, screen_width = init_screen. shape\n",
        "# Get number of actions from gym action space\n",
        "n_actions = env .action_space.n\n",
        "policy_net = DON(screen_height, screen_width, _actions). to(device)\n",
        "target_net = DON(screen _height, screen_width, _actions). to (device)\n",
        "target_net. load_state_dict(policy_net.state_dict))\n",
        "target_net.eval ()\n",
        "optimizer = optim.RMSprop(policy_net.parameters ())\n",
        "memory = ReplayMemory (10000)\n",
        "steps_done = 0"
      ],
      "metadata": {
        "id": "3SrUi6S4kVFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action (state):\n",
        "global steps_done\n",
        "sample = random. random()\n",
        "eps_threshold = EPS_END + (EPS_START - EPS_END) * H\n",
        "math.exp(-1\n",
        "* steps done / EPS DECAY)\n",
        "steps_done += 1\n",
        "if sample > eps_threshold:\n",
        "with torch.no_grad():\n",
        "# t. max (1) will return largest column value of each row.\n",
        "# second column on max result is index of where max element was\n",
        "# found, so we pick action with the larger expected reward.\n",
        "return policy_net (state) .max (1) [1]. view(1, 1)\n",
        "else:\n",
        "return torch. tensor ([[random.randrange (n_actions)]]. device=device, dtype=torch. long)"
      ],
      "metadata": {
        "id": "L2ZhQw_3kVIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize model ():\n",
        "if len (memory) < BATCH_ SIZE:\n",
        "return\n",
        "transitions = memory. sample (BATCH_ SIZE)\n",
        "# Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "# detailed explanation). This converts batch-array of Transitions\n",
        "# to Transition of batch-arrays.\n",
        "batch = Transition(*zip(*transitions))\n",
        "# Compute a mask of non-final states and concatenate the batch elements\n",
        "# (a final state would've been the one after which simulation ended)\n",
        "non_final_mask = torch. tensor (tuple(map(lambda s: s is not None.\n",
        "batch.next_state)). device=device. type=torch.boo\n",
        "non _final_next_states = torch.cat(ls for s in batch.next_state\n",
        "if s is not Nonel)\n",
        "state_batch = torch. cat (batch. state)\n",
        "action_batch = torch. cat (batch.action)\n",
        "reward_batch = torch.cat (batch.reward)\n",
        "# Compute Q(s_t, a)\n",
        "the model computes Q(s_t). then we select the\n",
        "# columns of actions taken. These are the actions which would've been taken\n",
        "# for each batch state according to policy_net\n",
        "state_action_values = policy_net (state_batch) .gather (1. action_batch)\n",
        "# Compute V(s_(t+1]) for all next states.\n",
        "# Expected values of actions for non_ final _next_states are computed based\n",
        "# on the \"older\" target_net: selecting their best reward with max (1) [0]\n",
        "# This is merged based on the mask, such that we Il have either the expected\n",
        "# state value or 0 in case the state was final.\n",
        "next_state_values = torch.zeros (BATCH_SIZE, device=device)\n",
        "next_state_values [non_final _mask] = target_net (non_final _next_states) . max (1) |[0] .detach ()\n",
        "# Compute the expected Q values\n",
        "expected_state_action_values = (next_state_values * GAMMA)\n",
        "+ reward batch\n",
        "# Compute Huber loss\n",
        "criterion = nn. SmoothL1Loss ()\n",
        "loss = criterion (state_action_values, expected state.\n",
        "values .unsqueeze(1))\n",
        "# Optimize the model\n",
        "optimizer.zero grad()\n",
        "loss .backward()\n",
        "for param in policy_net parameters) :\n",
        "param.grad. data.clamp_(-1, 1) optimizer .step()"
      ],
      "metadata": {
        "id": "7k8FAuxLkVJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400,900))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "vb2gTJKAmjuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### DQN Training\n",
        "num_episodes = 50\n",
        "for i_episode in range (num_episodes) :\n",
        "# Initialize the environment and state\n",
        "env.reset ()\n",
        "last_screen = get_screen ()\n",
        "current_screen = get_screen (\n",
        "state = current_screen - last_screen\n",
        "for + in count:\n",
        "# Select and perform an action\n",
        "action = select action(state)\n",
        "reward, done,_ = env.step(action.item())\n",
        "reward = torch. tensor ([reward], device=device)\n",
        "# Observe new state\n",
        "last_screen = current_screen\n",
        "current_screen = get_screen ()\n",
        "if not done:\n",
        "next_state = current_screen - last_screen\n",
        "else:\n",
        "next_state = None\n",
        "# Store the transition in memory\n",
        "memory.push (state, action, next_state, reward)\n",
        "# Move to the next state\n",
        "state = next state\n",
        "# Perform one step of the optimization (on the policy network)\n",
        "optimize_model ()\n",
        "if done:\n",
        "episode_durations. append (t + 1)\n",
        "plot_durations ( break\n",
        "# Update the target network, copying all weights and biases in DON\n",
        "if i_episode % TARGET_UPDATE == 0:\n",
        "target_net.load_state_dict(policy_net.state_dict())"
      ],
      "metadata": {
        "id": "SbOnrNILkVLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym. make ( 'CartPole-v0')\n",
        "env = gym.wrappers. Monitor (env, f\"videos\") # record videos\n",
        "env = gym. wrappers. RecordEpisodeStatistics(env) # record stats such as returns\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "  from Python import display\n",
        "pit.ion ()\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "wand. init(project=\"week14_dqn\", config=config. monitor_gym=True) \n",
        "wandb.run.name = \"dqn _cartpole\""
      ],
      "metadata": {
        "id": "0oI6fOcCkjK-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}